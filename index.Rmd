---
title       : FFTrees
subtitle    : An R package to create, visualize, and implement fast and frugal decision trees
author      : Nathaniel D. Phillips, Economic Psychology, University of Basel
job         : BaselR Meeting, March 2017, ndphillips.github.io/RBasel
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow      # 
widgets     : []            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
knit        : slidify::knit2slides
logo : FFTrees_Logo.jpg
biglogo : FFTrees_Logo.jpg
css        : slidify.css
---



```{r echo = FALSE, message = FALSE, results = "hide"}
load("../data/BaselR_FFTrees.RData")
load("../data/forensictrees.RData")
library(FFTrees)
library(knitr)
opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE, fig.retina = 2)
```

## A growing problem at the Cook County Hospital in 1996

```{r cookmap, fig.margin = TRUE, echo = FALSE, eval = TRUE, out.width = "70%", fig.align='center'}
knitr::include_graphics(c("images/cookmap.png"))
```

"As the city’s principal public hospital, Cook County was the place of last resort for the hundreds of thousands of Chicagoans without health insurance. Resources were stretched to the limit. The hospital’s cavernous wards were built for another century. There were no private rooms, and patients were separated by flimsy plywood dividers. There was no cafeteria or private telephone—just a payphone for everyone at the end of the hall. In one possibly apocryphal story, doctors once trained a homeless man to do routine lab tests because there was no one else available." Malcolm Gladwell, Blink.


--- .class #id 

## Emergency Room overload

```{r, fig.margin = TRUE, echo = FALSE, eval = TRUE, out.width = "60%", fig.align='center'}
knitr::include_graphics(c("images/crowdedemergency.jpg"))
```

- 250,000 patients per year
- Not enough space.
- Complete chaos


--- &twocol

*** =left

## Heart attacks (?)

<!-- A significant number of those people filing into the ED—on average, about thirty a day—were worried that they were having a heart attack.  Chest-pain patients were resource-intensive. The treatment protocol was long and elaborate and—worst of all—maddeningly inconclusive. -->

- 30 people a day worried about a heart attack
- **Coronary care bed** ($2,000 a night + 3 day stay) or **Regular bed**
- Goal: send true heart attacks to the coronary care bed, and true healthy patients to a normal bed.

### Multiple, uncertain measures

- Electrocardiogram (ECG), Blood pressure, Stethescope, How long? How much? During exercise? History? Cholesterol? Drugs? etc.


*** =right

```{r , fig.margin = TRUE, echo = FALSE, out.width = "80%", fig.align='center'}
knitr::include_graphics(c("images/paindecision.png"))
```

--- .class #id 

## How good were doctor's intuitive decisions?

- Task: Estimate from 0 to 100 the probability of a heart attack of 20 separate patients.

```{r, fig.margin = TRUE, echo = FALSE, eval = TRUE, out.width = "40%", fig.align='center'}
knitr::include_graphics(c("images/howlikelyquestion.png"))
```

--- .class #id 

## Answer: Not consistent at all

### "In each case the answers we got pretty much ranged from 0 to 100. It was extraordinary" ~ Department of Medicine chairman


### The problem

- Too much inconsistency in doctor's decisions
- Too many healthy people sent to the coronary care unit.



--- .class #id 

## Solution

- A fast and frugal decision tree (FFT) developed by a cardiologist named Lee Goldman.


```{r , fig.margin = TRUE, echo = FALSE, out.width = "20%", fig.align='center'}
knitr::include_graphics(c("images/cooktree.png"))
```


### Why use a decision tree?

> - Easy to understand, consistent, requires little information, can be calculated 'in the head'


--- .class #id 


```{r , fig.margin = TRUE, echo = FALSE, out.width = "40%", fig.align='center'}
knitr::include_graphics(c("images/cooktree.png"))
```


---  &twocol

*** =left

## The Cook hospital decision tree

- Over two years, the performance of the tree was compared to the physician's intuitive judgments.

### Results

> - Doctor's accuracy: 75-90%
> - Decision tree accuracy: 95%
> - Tree had far fewer false-positives and huge cost savings
> - To this day, the tree is still used at the hospital.

*** =right

```{r , fig.margin = TRUE, echo = FALSE, out.width = "80%", fig.align='center'}
knitr::include_graphics(c("images/cooktree.png"))
```




---  &twocol

*** =left

## Fast and Frugal tree

> - A fast and frugal decision tree (FFT) is a decision tree where each node has exactly two branches, where at least one branch is an exit branch (Martignon et al., 2008).

> - FFTs -> Cheap, easy to understand, and rarely overfit.

```{r , fig.margin = TRUE, echo = FALSE, out.width = "90%", fig.align='center', message = FALSE, echo = FALSE}
knitr::include_graphics(c("images/traintreestats.png"))
```

*** =right

## Standard decision tree

> - "Standard"" decision trees can become very complex.

> - Complexity -> High costs, Difficult to understand, prone to overfitting. 

```{r , fig.margin = TRUE, echo = FALSE, out.width = "100%", fig.align='center', message = FALSE, echo = FALSE}
knitr::include_graphics(c("images/complextree.png"))
```


<!-- --- .class #id  -->

<!-- ## Depression Tree -->

<!-- - Jenny et al. (2013): Simple rules for detecting depression -->

<!-- ```{r , fig.margin = TRUE, echo = FALSE, out.width = "40%", fig.align='center'} -->
<!-- knitr::include_graphics(c("images/depressiontree.png")) -->
<!-- ``` -->

<!-- --- .class #id  -->

<!-- ## Bank failure -->

<!-- - Neth et al. (2013): Homo heuristics in the financial world: From risk management to managing uncertainty -->

<!-- ```{r , fig.margin = TRUE, echo = FALSE, out.width = "40%", fig.align='center'} -->
<!-- knitr::include_graphics(c("images/nethtree.png")) -->
<!-- ``` -->

<!-- --- &twocol -->

<!-- ## Patient success -->

<!-- *** =left -->



<!-- ### FFT -->
<!-- ```{r echo = FALSE} -->
<!-- library(FFTrees) -->
<!-- plot(tree.63.m, stats = FALSE, tree = 5) -->
<!-- ``` -->



<!-- *** =right -->


<!-- ### rpart tree -->
<!-- ```{r echo = FALSE} -->
<!-- plot(rpart.63) -->
<!-- text(rpart.63) -->
<!-- ``` -->

--- .class #id 
## Problem

- There is no off-the-shelf method to construct FFTs.
  - Previous researchers have individually constructed their FFTs.

### Task
- Create an easy-to-use R package that constructs, visualizes, and implements FFTs.

```{r , fig.margin = TRUE, echo = FALSE, out.width = "30%", fig.align='center', message = FALSE, echo = FALSE}
knitr::include_graphics(c("images/FFTrees_Logo.jpg"))
```


--- .class #id 
## FFTrees

```{r, eval = FALSE}
# v1.1.8 available on CRAN
install.packages("FFTrees")

# v1.2.0 on github
devtools::github("ndphillips/FFTrees", include_vignette = TRUE)
```


--- .class #id 
## Heart disease datatset

```{r, message = FALSE}
library(FFTrees)

head(heartdisease)
```

--- .class #id 
## 4 Steps to using FFTrees

```{r, message = FALSE, eval = FALSE}
# Step 1: Create the trees
heart.fft <- FFTrees(formula = diagnosis ~., 
                     data = heart.train,
                     data.test = heart.test)

# Step 2: View summary statistics
print(heart.fft)

# Step 3: Visualise the tree
plot(heart.fft, data = "train")   # Training statistics
plot(herat.fft, data = "test")    # Test statistics
```


```{r echo = FALSE}
# Step 0: Create training and test data
set.seed(100)

heartdisease <- heartdisease[sample(nrow(heartdisease)),]
heart.train <- heartdisease[1:150,]
heart.test <- heartdisease[151:303,]
```


<!-- --- .class #id  -->
<!-- ## Evaluating a decision algorithm -->


<!-- ```{r , fig.margin = TRUE, echo = FALSE, out.width = "60%", fig.align='center'} -->
<!-- knitr::include_graphics(c("images/confusiontable.png")) -->
<!-- ``` -->




--- .class #id 
## Printing an FFTrees object

```{r, message = FALSE, echo = TRUE}
# Step 2: Summary statistics
heart.fft
```

--- &twocol

*** =left

## Plotting an FFTrees object

`plot(heart.fft, stats = FALSE)`

```{r, eval = TRUE, fig.align = 'center', echo = FALSE}
plot(heart.fft, 
     main = "Heart Disease", 
     decision.names = c("Healthy", "Diseased"), 
     stats = FALSE)
```



*** =right

### 3 cues


| cue| cost | description|values |
|:------|:---|:----|:-----|
|     `thal`| $102 | thallium scintigraphy, a nuclear imaging test that shows how well blood flows into the heart.|normal (n), fixed defect (fd), reversible defect (rd)     |
|     `cp`| $1 |    Chest pain type| Typical angina (ta), atypical angina (aa), non-anginal pain (np), asymptomatic (a)     |
|     `ca`| $101 | Number of major vessels colored by flourosopy, a continuous x-ray imaging tool|0, 1, 2 or 3 |


--- .class #id 
#### `plot(heart.fft)`
```{r, eval = TRUE, echo = FALSE, fig.align = "center", dpi = 300, fig.width = 8, fig.height = 7, out.width = "600px"}
plot(heart.fft, main = "Heart Disease", decision.names = c("healthy", "sick"))
```



--- .class #id 
#### `plot(heart.fft, data = "test")`

```{r, eval = TRUE, echo = FALSE, fig.align = "center",, dpi = 300, fig.width = 8, fig.height = 7, out.width = "600px"}
plot(heart.fft, main = "Heart Disease", decision.names = c("healthy", "sick"), data = "test")
```


--- .class #id 
## Heart Disease FFT | ROC

```{r , fig.margin = TRUE, echo = FALSE, out.width = "80%", fig.align='center'}
knitr::include_graphics(c("images/roc.png"))
```


--- .class #id 
#### `plot(heart.fft, data = "test")`

```{r, eval = TRUE, echo = FALSE, fig.align = "center", dpi = 300, fig.width = 8, fig.height = 7, out.width = "600px"}
plot(heart.fft, 
     main = "Heart Disease", 
     decision.names = c("healthy", "sick"), 
     data = "test")
```



--- .class #id 
#### `plot(heart.fft, data = "test", tree = 3)`

```{r, message = FALSE, fig.align = 'center', echo = FALSE, message = FALSE, results = "hide", dpi = 300, fig.width = 8, fig.height = 7, out.width = "600px"}

plot(heart.fft, main = "Heart Disease", 
     decision.names = c("healthy", "sick"), data = "test", tree = 3)
```

--- .class #id
#### `plot(heart.fft, data = "test", tree = 6)`

```{r, message = FALSE, fig.align = 'center', echo = FALSE, message = FALSE, results = "hide", dpi = 300, fig.width = 8, fig.height = 7, out.width = "600px"}

plot(heart.fft, main = "Heart Disease", 
     decision.names = c("healthy", "sick"), data = "test", tree = 6)
```

<!-- --- .class #id  -->
<!-- ## How do FFTs compare to regression and CART? -->

<!-- - Simplicity: *How much information is used and how is it combined?* -->
<!-- - Accuracy: *How well can the algorithm predict new data?* -->

<!-- --- .class #id  -->
<!-- ## Heart disease: regression -->
<!-- - 4 significant cues: (sex, cp, trestbps, ca) -->

<!-- ```{r echo = FALSE, fig.width = 10, fig.height = 6, fig.align = 'center'} -->
<!-- heart.lm <- glm(diagnosis ~., data = heartdisease, family = "binomial") -->
<!-- #heart.lm$coefficients -->
<!-- #summary(heart.lm)$coefficients -->

<!-- bar.cols <- rep("white", nrow(summary(heart.lm)$coefficients)) -->
<!-- bar.cols[summary(heart.lm)$coefficients[,4] < .05] <- yarrr::piratepal("basel", trans = .6, length.out = sum(summary(heart.lm)$coefficients[,4] < .05)) -->

<!-- barplot(height = abs(summary(heart.lm)$coefficients[,3]),  -->
<!--         names.arg = rownames(summary(heart.lm)$coefficients), -->
<!--         col = bar.cols, ylab = "Coefficient z-scores", main = "Heart Disease logistic regression") -->

<!-- abline(h = 1.96) -->

<!-- ``` -->



--- .class #id 
## Comparing FFTs to standard trees


### How does the FFT created by FFTrees compare to a 'standard' decision tree created by rpart?


--- &twocol

*** =left
## Heart disease: rpart

- 8 predictors, 3 - 5 required to make decisions

```{r, out.width = "80%", echo = FALSE, fig.align = 'center'}
heart.rpart <- rpart::rpart(diagnosis ~., data = heartdisease)
plot(heart.rpart)
text(heart.rpart)
```


**** =right
## Heart disease: FFT

- 3 predictors, only 1 - 3 required to make decisions

- The FFT is very cheap to implement
    - Regression: $300
    - rpart: > $100
    - Heart disease FFT: $75.91



```{r , fig.margin = TRUE, echo = FALSE, out.width = "70%", fig.align='center', message = FALSE, echo = FALSE}
knitr::include_graphics(c("images/traintreestats.png"))
```



--- .class #id
## Heart disease classification accuracy

```{r, echo = FALSE,  fig.align = 'center', out.width = "80%", message = FALSE, results = "hide"}
heart.sim <- data.frame(model = c(rep("FFT", 100),
                                  rep("Regression", 100),
                                  rep("rpart", 100)),
                        data = c(rep("train", 300), rep("test", 300)),
                        bacc = c(heart.fff$tree.sim$bacc.train,
                                          heart.fff$lr.sim$bacc.train,
                                          heart.fff$cart.sim$bacc.train,
                                 heart.fff$tree.sim$bacc.test,
                                          heart.fff$lr.sim$bacc.test,
                                          heart.fff$cart.sim$bacc.test))
```


```{r, echo = FALSE,  fig.align = 'center', fig.width = 10, fig.height = 7, message = FALSE, results = "hide"}
yarrr::pirateplot(bacc ~ model + data, data = heart.sim, ylim = c(.6, 1), sortx = "s", main = "Simulation performance in Heart disease", ylab = "Balanced Accuracy (bacc)", yaxt = "n", gl.col = "white")
axis(side = 2, at = seq(.6, 1, .1), las = 1)
rect(-1000, -1000, 10000, 1000, col = "white", border = NA)
grid()
```



--- .class #id
## Heart disease classification accuracy

```{r, echo = FALSE,  fig.align = 'center', fig.width = 10, fig.height = 7, message = FALSE, results = "hide"}
yarrr::pirateplot(bacc ~ model + data, data = heart.sim, ylim = c(.6, 1), sortx = "s", main = "Simulation performance in Heart disease", ylab = "Balanced Accuracy (bacc)", yaxt = "n", gl.col = "white")
axis(side = 2, at = seq(.6, 1, .1), las = 1)
rect(4, -1000, 10000, 1000, col = "white", border = NA)
grid()
```

--- .class #id
## Heart disease classification accuracy


```{r, echo = FALSE,  fig.align = 'center', fig.width = 10, fig.height = 7, message = FALSE, results = "hide"}
yarrr::pirateplot(bacc ~ model + data, data = heart.sim, ylim = c(.6, 1), sortx = "s", main = "Simulation performance in Heart disease", ylab = "Balanced Accuracy (bacc)", yaxt = "n", gl.col = "white")
axis(side = 2, at = seq(.6, 1, .1), las = 1)
grid()
```


--- .class #id 
## How accurate are FFTs built by FFTrees?

- Prediction competition
    - 10 datasets taken from the UCI machine learning database
    - 50% Fitting / 50% Prediction subsample splitting, DV: balanced accuracy = (sensitivity + specificity) / 2

|dataset     | cases| cues| base.rate|
|:-----------|-----:|----:|---------:|
|arrhythmia  |    68|  280|      0.29|
|audiology   |   226|   70|      0.10|
|breast      |   683|   10|      0.35|
|bridges     |    92|   10|      0.39|
|cmc         |  1473|   10|      0.35|

Table: 5 of the 10 prediction datasets

--- .class #id 
## Aggregate simulation prediction results

```{r , fig.margin = TRUE, echo = FALSE, out.width = "90%", fig.align='center'}
knitr::include_graphics(c("images/simulationagg_a.png"))
```


--- .class #id 
## Aggregate simulation prediction results

```{r , fig.margin = TRUE, echo = FALSE, out.width = "90%", fig.align='center'}
knitr::include_graphics(c("images/simulationagg_b.png"))
```

--- .class #id 
## Aggregate simulation prediction results

```{r , fig.margin = TRUE, echo = FALSE, out.width = "90%", fig.align='center'}
knitr::include_graphics(c("images/simulationagg_c.png"))
```


--- .class #id 
## Simulation prediction results by dataset

```{r , fig.margin = TRUE, echo = FALSE, out.width = "90%", fig.align='center'}
knitr::include_graphics(c("images/simulation.png"))
```

--- &twocol

*** =left

## Conclusions

- FFTrees makes it easy to create simple, effective, transparent fast and frugal decision trees (FFTs).

- FFTs can predict data "as well" as complex algorithms that use much more information.

## Next steps

- Speed up code with c++ or Julia.
- Include *cue costs* into algorithm.
- Quantify when a tree **fails** over time.

*** =right

```{r eval = FALSE}
# Create FFTs in one line of code
FFTrees(diagnosis ~., 
        data = heartdisease)
```

```{r, eval = TRUE, fig.align = 'center', echo = FALSE}
plot(heart.fft, main = "Heart Disease", 
     decision.names = c("Healthy", "Diseased"), stats = FALSE)
```


--- 

## Please help and contribute!

- I am very happy for contributions and bug reports at [http://www.github.com/ndphillips/FFTrees](http://www.github.com/ndphillips/FFTrees), 

- If you have data you want to try `FFTrees` on, or can think of new features, let's collaborate!


[](https://www.revive-adserver.com/media/GitHub.jpg)

```{r , fig.margin = TRUE, echo = FALSE, out.width = "80%", fig.align='center', message = FALSE, echo = FALSE}
knitr::include_graphics("https://www.revive-adserver.com/media/GitHub.jpg")
```


--- &twocol

*** =left

## Questions?

```{r, eval = TRUE, fig.width = 8, fig.height = 7, fig.align='center', echo = FALSE, out.width = "85%"}
plot(heart.fft, 
     main = "Heart Disease", 
     decision.names = c("healthy", "sick"))
```

*** =right

## Contact

### Website
### http://ndphillips.github.io

### Email
### Nathaniel.D.Phillips.is@gmail.com


### This talk
### http://ndphillips.github.io/BaselR


--- .class #id 

## Fitting vs. Prediction

```{r echo = FALSE}
set.seed(101)
q <- sort(runif(50, min = 0, 20))
noise.train <- rnorm(length(q), mean=10, sd=120)
noise.test <- rnorm(length(q), mean=10, sd=120)

signal <- 500 + 0.4 * (q-10) ^ 3

y.train <- signal + noise.train
y.test <- signal + noise.test

model.1 <- lm(y.train ~ poly(q,1))
model.3 <- lm(y.train ~ poly(q,3))
model.20 <- lm(y.train ~ poly(q,20))

model.1.train.pred <- predict(model.1, data.frame(x = y.train))
model.3.train.pred <- predict(model.3, data.frame(x = y.train))
model.20.train.pred <- predict(model.20, data.frame(x = y.train))

model.1.train.mae <- mean(abs(model.1.train.pred - y.train))
model.3.train.mae <- mean(abs(model.3.train.pred - y.train))
model.20.train.mae <- mean(abs(model.20.train.pred - y.train))

model.1.test.pred <- predict(model.1, data.frame(x = y.test))
model.3.test.pred <- predict(model.3, data.frame(x = y.test))
model.20.test.pred <- predict(model.20, data.frame(x = y.test))

model.1.test.mae <- mean(abs(model.1.test.pred - y.test))
model.3.test.mae <- mean(abs(model.3.test.pred - y.test))
model.20.test.mae <- mean(abs(model.20.test.pred - y.test))

poly.stats <- cbind(c(model.1.train.mae, model.3.train.mae, model.20.train.mae),
                    c(model.1.test.mae, model.3.test.mae, model.20.test.mae))

colnames(poly.stats) <- c("train", "test")
rownames(poly.stats) <- c("p1", "p3", "p20")
```


```{r echo = FALSE,   fig.width = 12, fig.height = 6}
my.cols <- yarrr::piratepal("xmen", trans = .3)[c(3, 1, 2)]

par(mfrow = c(1, 2))

plot(q,y.train,col='deepskyblue4',xlab='q',main='Observed data', ylim = c(0, 1000))
lines(q,signal,col="black",lwd=3, lty = 2)


poly.stats.3 <- poly.stats
poly.stats.3[,1:2] <- 0

legend("topleft", 
       legend = c("Simple", "Perfect", "Complex"), 
       col = my.cols, lty = 1, lwd = 3)

barplot(poly.stats.3, beside = TRUE, ylim = c(50, 150), xpd = F, col = my.cols[1:3], ylab = "Error (higher is worse!)")

legend("topright", 
       legend = c("Simple", "Perfect", "Complex"), 
       col = my.cols, lty = 1, lwd = 3)
```



--- .class #id 

## Fitting vs. Prediction

```{r echo = FALSE,  fig.width = 12, fig.height = 6}
my.cols <- yarrr::piratepal("xmen", trans = .3)[c(3, 1, 2)]

par(mfrow = c(1, 2))

plot(q,y.train,col='deepskyblue4',xlab='q',main='Observed data', ylim = c(0, 1000))
lines(q,signal,col="black",lwd=3, lty = 2)

lines(q, model.1.train.pred, col=my.cols[1],lwd=5)
lines(q, model.3.train.pred, col=my.cols[2],lwd=5)
lines(q, model.20.train.pred, col=my.cols[3],lwd=5)

poly.stats.2 <- poly.stats
poly.stats.2[,1:2] <- 0

legend("topleft", 
       legend = c("Simple", "Perfect", "Complex"), 
       col = my.cols, lty = 1, lwd = 3)

barplot(poly.stats.2, beside = TRUE, ylim = c(50, 150), xpd = F, col = my.cols[1:3], ylab = "Error (higher is worse!)")

legend("topright", 
       legend = c("Simple", "Perfect", "Complex"), 
       col = my.cols, lty = 1, lwd = 3)

```


--- .class #id 

## Fitting vs. Prediction

```{r echo = FALSE,  fig.width = 12, fig.height = 6}
my.cols <- yarrr::piratepal("xmen", trans = .3)[c(3, 1, 2)]

par(mfrow = c(1, 2))

plot(q,y.train,col='deepskyblue4',xlab='q',main='Observed data', ylim = c(0, 1000))
lines(q,signal,col="black",lwd=3, lty = 2)

lines(q, model.1.train.pred, col=my.cols[1],lwd=5)
lines(q, model.3.train.pred, col=my.cols[2],lwd=5)
lines(q, model.20.train.pred, col=my.cols[3],lwd=5)

poly.stats.2 <- poly.stats
poly.stats.2[,2] <- 0

legend("topleft", 
       legend = c("Simple", "Perfect", "Complex"), 
       col = my.cols, lty = 1, lwd = 3)

barplot(poly.stats.2, beside = TRUE, ylim = c(50, 150), xpd = F, col = my.cols[1:3], ylab = "Error (higher is worse!)")

legend("topright", 
       legend = c("Simple", "Perfect", "Complex"), 
       col = my.cols, lty = 1, lwd = 3)

```


--- .class #id 

## Fitting vs. Prediction

```{r echo = FALSE, fig.width = 12, fig.height = 6}
my.cols <- yarrr::piratepal("xmen", trans = .3)[c(3, 1, 2)]

par(mfrow = c(1, 2))

plot(q,y.test,col='deepskyblue4',xlab='q',main='Observed data', ylim = c(0, 1000), pch = 2)

points(q,y.train,col=yarrr::transparent('deepskyblue4', .6))


lines(q,signal,col="black",lwd=3)

lines(q,model.1.test.pred,col=my.cols[1],lwd=5)
lines(q,model.3.test.pred,col=my.cols[2],lwd=5)
lines(q,model.20.test.pred,col=my.cols[3],lwd=5)

legend("topleft", 
       legend = c("Simple", "Perfect", "Complex"), 
       col = my.cols, lty = 1, lwd = 3)

barplot(poly.stats, beside = TRUE, ylim = c(50, 150), xpd = F, col = my.cols[1:3], ylab = "Error (higher is worse!)")

legend("topright", 
       legend = c("Simple", "Perfect", "Complex"), 
       col = my.cols, lty = 1, lwd = 3)
```




--- &twocol

*** =left
## FFTrees algorithm

1. Calculate a decision threshold `t` for each cue that maximizes the cue’s balanced accuracy `bacc` in training.

2. Rank cues in order of their maximum balanced accuracy -- select the top N cues. 

3. Creates all possible `2^{N−1}` trees with these cues, using all exit structures.

*** =right

```{r echo = FALSE, fig.align = 'center'}
heartdisease$chol.cut <- cut(heartdisease$chol, breaks = 10)

cutoffs <- seq(min(heartdisease$chol), max(heartdisease$chol), length.out = 10)

vals <- sapply(cutoffs, FUN = function(x) {
  
  acc <- mean(heartdisease$diagnosis[heartdisease$chol >= x]) * .5 +
         mean(heartdisease$diagnosis[heartdisease$chol < x] == FALSE) * .5
  
})


plot(cutoffs, vals, ylab = "bacc", xlab = "Decision Threshold", main = "Cholestorol", type = "b", ylim = c(0, 1))

text(cutoffs, vals, paste(">", round(cutoffs, 0)), pos = 3)

with(subset(heartdisease, diagnosis == 1), 
     points(chol, rep(.9, length(chol)) + rnorm(length(chol), mean = 0, sd = .02), pch = 21, bg = yarrr::transparent("red", .6), col = "white"))

with(subset(heartdisease, diagnosis == 0), 
     points(chol, rep(.8, length(chol)) + rnorm(length(chol), mean = 0, sd = .02), pch = 21, bg = yarrr::transparent("blue", .6), col = "white"))

```



--- .class #id 
## FFForest()

### Visualise cue importance and co-occurence

```{r, fig.align = 'center', echo = FALSE, dpi = 300, out.width = "800px", fig.width = 12, fig.height = 6}
plot(heart.fff)
```



--- .class #id 
## Heart disease cue accuracies

```{r eval = TRUE, fig.align = 'center'}
plot(heart.fft, what = "cues", main = "Heart Disease")
```



